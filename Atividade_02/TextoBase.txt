1= introdução;
2= problema;
3 = solução;
4 = trabalho;
5 = resultados;


5º artigo 

1= Software testing is a key practice in software development. As a
proxy of test quality, one can rely on code coverage or mutation
analysis. Coverage measures the percentage of code that is covered
by tests and is typically used to assess the test effectiveness.

Test mutation technique assesses test effectiveness in four major
steps (illustrated in Figure 1-left). First, the project test suite is
executed and the results are stored as the expected output. Then, a
mutation testing engine (e.g., PIT [26]) parses the code and applies
mutation operators on code structures generating a set of mutants.
The mutants are separately tested by the test suite and the results
form the obtained output. Lastly, each obtained output is compared
to the expected output. A mutant is “killed” when at least one of
the test results differs between both sets, i.e., when at least one of
the test methods run on the mutants failed, meaning they properly
detected the code mutations. Finally, a mutation score is computed:
higher scores mean the test suite is better in catching real bugs [34].


2= However, it presents some limitations [5, 21, 34]. For example,
one can have great coverage with no assert. Although test suite mutation testing is ideal for gathering the overall
test quality in a system, it has three limitations: (1) the overall
system mutation score overshadows the quality of individual test
methods; (2) the quality of a contribution (e.g., a pull request with
code and tests) can be unnoticed in a large system because its score
may be unaffected by small code changes (due to existing tests
outnumber the contributed tests); (3) existing test methods may
kill mutants within a contribution and hinder assessing the quality
of the contributed tests.


4=We collect the top 15 Java repositories from GitHub (in terms of
the star metric [3, 28]) and the Apache Commons Lang. Next, for
each project, we clone the latest master branch version, manually
configure the extended PIT [26] via their build configuration file,
and discard the projects accused by PIT of having failing tests. The
five remaining projects are highly active and their size ranges from
35.9KLOC (Retrofit) to 310.8KLOC (RxJava).

After selecting the target projects, we start the mutation testing
execution phase. Table 1 summarizes this analysis: in total, PIT
detected 18,321 test cases in the five projects. Overall, it generated
55,427 mutants, which resulted in 16,149,383 mutant executions.
The mutation scores are overall high, ranging from 73% (Okhttp) to
86% (Commons Lang). For comparison purposes,we also present the
coverage values in the last column. As expected [34], the coverage
values are frequently higher than the mutation score.

The next step is to select the test methods to be analyzed. To be
selected for this study, test methods must: (1) contain a @Test annotation
or a name prefixed by test, (2) not rely on anonymous classes,
(3) not contain neither @Ignore nor @Disabled annotations, and (4)
have a mutation score computable by PIT, i.e., it covers at least one
mutant. Next, we collect the mutation score of the test methods

5= We show
empirical evidence that there are no major differences between
high-quality and low-quality test methods in terms of size, number
of asserts, and modifications. Low-quality test methods are overconcentrated
on critical test smells, while high-quality test methods
are likely to contain less important ones.
As future work, we plan to extend our dataset and to include
more static and runtime metrics in the analysis to better assess test
quality, e.g., metrics related to the adopted assertions and the input
data. Lastly, we plan to provide more qualitative analysis on the
differences between high and low-quality test methods.