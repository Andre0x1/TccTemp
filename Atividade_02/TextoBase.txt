1= introdução;
2= problema;
3 = solução;
4 = trabalho;
5 = resultados;


5º artigo 

1 = Code Review is the process of analyzing source code written
by a teammate to judge whether it is of sufficient quality
to be integrated into the main code trunk. Recent studies
provided evidence that reviewed code has lower chances of
being buggy [1]–[3] and exhibit higher internal quality [3],
likely being easier to comprehend and maintain. Given these
benefits, code reviews are widely adopted both in industrial
and open source projects with the goal of finding defects,
improving code quality, and identifying alternative solutions.
The benefits brought by code reviews do not come for
free. Indeed, code reviews add additional expenses to the
standard development costs due to the allocation of one or
more reviewers having the responsibility of verifying the
correctness, quality, and soundness of newly developed code.


2 = Bosu and Carver report that developers spend, on average,
more than six hours per week reviewing code [4].This is
not surprising considering the high number of code changes
reviewed in some projects: Rigby and Bird [5] show that industrial
projects, such as Microsoft Bing, can undergo thousands
of code reviews per month (3k in the case of Bing). Also, as
highlighted by Czerwonka et al. [6], the effort spent in code
review does not only represent a cost in terms of time, but
also pushes developers to switch context from their tasks.


3 = In this paper, we make a first step in this direction by using
Deep Learning (DL) models to partially automate specific code
review tasks.


We built two crawlers for mining from Gerrit and GitHub
code review data. Before moving to the technical details, it
is important to clarify what the goal of this mining process
is. Once a code contribution (i.e., changes impacting a set of
existing code files or resulting in new files) is submitted for
review, it can be a subject to several review rounds. Let us
assume that Cs is the set of code files submitted for review,
since subject to code changes.




4=  First, from the perspective of the contributor
(i.e., the developer submitting the code for review), we train
a transformer model [7] to “translate” the code submitted
for review into a version implementing code changes that a
reviewer is likely to suggest. In other words, we learn code
changes recommended by reviewers during review activities
and we try to automatically implement them on the code
submitted for review. This could give a fast and preliminary
feedback to the contributor as soon as she submits the code.
This model has been trained on 17,194 code pairs of Cs ! Cr
where Cs is the code submitted for review and Cr is the code
implementing a specific comment provided by the reviewer.


 Construct validity. While we applied many heuristics to
clean the data used in the training and testing of the NMT
model, by manual inspecting our dataset we still noticed
a small percentage of “noisy” comments (i.e., reviewers’
comments unlikely to trigger code changes).

Internal validity. Subjectiveness in the manual analyses
could have potentially affected our results. To mitigate such a
bias, when classifying comments as relevant or irrelevant, two
authors independently classified each comment, and a third
author was involved in a case of a conflict.

External validity. We mined our datasets from both Gerrit
and GitHub, considering a large set of projects (8,904).
However, we only focused on Java systems, thus limiting
the generalizability of our findings. 



5= The achieved results were promising, with the models able
to generate meaningful recommendations in up to 16% (first
scenario) and 31% (second scenario) of cases. Still, vast
improvements are needed to make such models suitable to
be used by developers.

